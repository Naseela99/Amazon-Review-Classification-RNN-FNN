{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fdfa61f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "406e6595",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "406e6595",
    "outputId": "adeea700-c6b8-48f7-8de4-18190a5aa57a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/naseela/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "dl = nltk.downloader.Downloader('https://www.nltk.org/nltk_data/')\n",
    "dl.download('wordnet')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from gensim import utils\n",
    "import gensim.downloader\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b109df",
   "metadata": {},
   "source": [
    "## 1. Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e03f65",
   "metadata": {},
   "source": [
    "Since we are just using the **review_body** and **star_rating** for this assignment, I am reading just those columns. I have also renamed the classes as 0,1,2 for simplification in the Pytorch models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90725c4a",
   "metadata": {
    "id": "90725c4a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naseela/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_table('data.tsv', on_bad_lines = 'skip',verbose = False,usecols=['review_body','star_rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa21602f",
   "metadata": {
    "id": "fa21602f"
   },
   "outputs": [],
   "source": [
    "new_data = data[['review_body','star_rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeaf5c4f",
   "metadata": {
    "id": "aeaf5c4f"
   },
   "outputs": [],
   "source": [
    "new_data = new_data.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49c9fea1",
   "metadata": {
    "id": "49c9fea1"
   },
   "outputs": [],
   "source": [
    "# Changing classes to the required 3 classes\n",
    "classes = []\n",
    "star_ratings = new_data['star_rating'].to_list()\n",
    "\n",
    "for rating in star_ratings:\n",
    "\n",
    "  if int(rating) == 1 or int(rating) == 2:\n",
    "    classes.append(0)\n",
    "  elif int(rating) == 3:\n",
    "    classes.append(1)\n",
    "  else:\n",
    "    classes.append(2)\n",
    "\n",
    "new_data['class'] = classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51d7e24a",
   "metadata": {
    "id": "51d7e24a"
   },
   "outputs": [],
   "source": [
    "# Sampling from the classes\n",
    "random_sample_class1 = new_data[new_data['class']==0].sample(n=20000,replace=False)\n",
    "random_sample_class2 = new_data[new_data['class']==1].sample(n=20000,replace=False)\n",
    "random_sample_class3 = new_data[new_data['class']==2].sample(n=20000,replace=False)\n",
    "\n",
    "class_samples = [random_sample_class1,random_sample_class2,random_sample_class3]\n",
    "\n",
    "df_class = pd.concat(class_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181ea4b8",
   "metadata": {},
   "source": [
    "As mentioned in the assignment, the train-test split is 80/20. I am following the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb652f8b",
   "metadata": {
    "id": "fb652f8b"
   },
   "outputs": [],
   "source": [
    "# Train test split\n",
    "\n",
    "X = df_class['review_body']\n",
    "\n",
    "y = df_class['class']\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e029de",
   "metadata": {
    "id": "a6e029de"
   },
   "source": [
    "## 2. Word Embedding (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeba7c3",
   "metadata": {
    "id": "adeba7c3"
   },
   "source": [
    "### (a) (5 points)\n",
    "Load the pretrained “word2vec-google-news-300” Word2Vec model and learn\n",
    "how to extract word embeddings for your dataset. Try to check semantic\n",
    "similarities of the generated vectors using three examples of your own, e.g.,\n",
    "King − M an + Woman = Queen or excellent ∼ outstanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6eab5c",
   "metadata": {},
   "source": [
    "For this part, I have created a function, that extracts the embeddings if the word is in the vocab, otherwise it returns a default vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f17cb3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the vector for a word\n",
    "def get_vector(model,word):\n",
    "    default = [0.0] * model.vector_size\n",
    "    try:\n",
    "        return model[word]\n",
    "    except KeyError:\n",
    "        return default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caebd85d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "caebd85d",
    "outputId": "a3421e82-fa8b-4238-aa8d-bfb94cdf32d3"
   },
   "outputs": [],
   "source": [
    "w2v_g300 = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e21ea637",
   "metadata": {
    "id": "fbe06544"
   },
   "source": [
    "**Example 1: amazing = awesome**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45917ff4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45917ff4",
    "outputId": "0b9f33b1-78b7-428b-d25a-ca22235e3f1b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between \"amazing\" and \"awesome\" =  0.8282866\n"
     ]
    }
   ],
   "source": [
    "vec_amazing, vec_awesome= w2v_g300['amazing'],w2v_g300['awesome']\n",
    "\n",
    "\n",
    "# Reference = https://www.geeksforgeeks.org/how-to-calculate-cosine-similarity-in-python/\n",
    "print('Cosine similarity between \"amazing\" and \"awesome\" = ',np.dot(vec_amazing,vec_awesome)/(norm(vec_amazing)*norm(vec_awesome)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58da0de9",
   "metadata": {
    "id": "58da0de9"
   },
   "source": [
    "**Example 2: very angry = upset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d80c8083",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d80c8083",
    "outputId": "f4e4cd93-d9c8-465b-9eff-d1629149ff67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between \"very angry\" and \"upset\" =  0.5078693\n"
     ]
    }
   ],
   "source": [
    "vec_very, vec_angry, vec_furious = w2v_g300['very'],w2v_g300['angry'],w2v_g300['upset']\n",
    "\n",
    "lhs = (vec_very + vec_angry)/2\n",
    "\n",
    "# Reference = https://www.geeksforgeeks.org/how-to-calculate-cosine-similarity-in-python/\n",
    "print('Cosine similarity between \"very angry\" and \"upset\" = ',np.dot(lhs,vec_furious)/(norm(lhs)*norm(vec_furious)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab627ec5",
   "metadata": {
    "id": "ab627ec5"
   },
   "source": [
    "**Example 3: pretty = ugly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1faa0d93",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1faa0d93",
    "outputId": "7da520bb-690c-4dcc-e151-4d6e29f494b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between \"pretty\" and \"ugly\" =  0.2727458\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vec_pretty, vec_ugly = w2v_g300['pretty'],w2v_g300['ugly']\n",
    "\n",
    "\n",
    "lhs = vec_pretty\n",
    "\n",
    "# Reference = https://www.geeksforgeeks.org/how-to-calculate-cosine-similarity-in-python/\n",
    "print('Cosine similarity between \"pretty\" and \"ugly\" = ',np.dot(lhs,vec_ugly)/(norm(lhs)*norm(vec_ugly)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c2b4ec",
   "metadata": {
    "id": "97c2b4ec"
   },
   "source": [
    "### (b) (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d4af5f",
   "metadata": {
    "id": "59d4af5f"
   },
   "source": [
    "### Train a Word2Vec model using your own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "658d3d63",
   "metadata": {
    "id": "658d3d63"
   },
   "outputs": [],
   "source": [
    "reviews = df_class.review_body.apply(lambda x: [w for w in x.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4d6dc98",
   "metadata": {
    "id": "d4d6dc98"
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences = reviews, vector_size = 300, window = 13,min_count=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d1a20c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2d1a20c8",
    "outputId": "5dec3000-11da-433b-98f1-509f8362d114"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between \"amazing\" and \"awesome\" =  0.81626076\n"
     ]
    }
   ],
   "source": [
    "vec_amazing, vec_awesome= model.wv['amazing'],model.wv['awesome']\n",
    "\n",
    "lhs = vec_amazing\n",
    "rhs =  vec_awesome\n",
    "\n",
    "# Reference = https://www.geeksforgeeks.org/how-to-calculate-cosine-similarity-in-python/\n",
    "print('Cosine similarity between \"amazing\" and \"awesome\" = ',np.dot(lhs,rhs)/(norm(lhs)*norm(rhs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ca5fddf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ca5fddf",
    "outputId": "6c82c8f8-939d-4209-9298-3fec804d6a4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between \"very angry\" and \"upset\" =  0.016726496\n"
     ]
    }
   ],
   "source": [
    "vec_very, vec_angry, vec_furious = model.wv['very'],model.wv['angry'], model.wv['upset']\n",
    "\n",
    "lhs = (vec_very + vec_angry)/2\n",
    "\n",
    "# Reference = https://www.geeksforgeeks.org/how-to-calculate-cosine-similarity-in-python/\n",
    "print('Cosine similarity between \"very angry\" and \"upset\" = ',np.dot(lhs,vec_furious)/(norm(lhs)*norm(vec_furious)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ef151bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ef151bb",
    "outputId": "ec638b7a-e4d3-4823-f34e-c67362a0d66b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between \"pretty\" and \"ugly\" =  0.3070316\n"
     ]
    }
   ],
   "source": [
    "vec_pretty, vec_ugly = model.wv['pretty'],model.wv['ugly']\n",
    "\n",
    "lhs = vec_pretty\n",
    "\n",
    "# Reference = https://www.geeksforgeeks.org/how-to-calculate-cosine-similarity-in-python/\n",
    "print('Cosine similarity between \"pretty\" and \"ugly\" = ',np.dot(lhs,vec_ugly)/(norm(lhs)*norm(vec_ugly)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527f86c0",
   "metadata": {},
   "source": [
    "``` (a). What do you conclude from comparing vectors\n",
    "generated by yourself and the pretrained model? Which of the Word2Vec\n",
    "models seems to encode semantic similarities between words better?```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682edd0f",
   "metadata": {},
   "source": [
    "From the results above, I noticed that the results of the model trained on my dataset performs worse than the Google-300 pre-trained model. For calculating the semantic similarities, I have used the cosine distance between the vectors. The vector of one sentence is taken using the mean of all the word2vec embeddings.\n",
    "\n",
    "In conclusion, the pretrained model performs better than the model I have trained. This is expected as the pretraiend model has been trained on a larger corpus. The Google-300 pretrained model encodes the semantic similarities between words better than our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280be959",
   "metadata": {
    "id": "280be959"
   },
   "source": [
    "## 3. Simple models (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e87d78f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e87d78f5",
    "outputId": "c64171d8-484e-4023-f6a0-0e5cff32322c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naseela/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/naseela/anaconda3/lib/python3.9/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "def get_features(df):\n",
    "    features = []\n",
    "    reviews = df.apply(lambda x: [w for w in x.split()])\n",
    "    for review in reviews:\n",
    "        rev_vec = []\n",
    "        for w in review:\n",
    "\n",
    "            if w  in w2v_g300:\n",
    "                rev_vec.append(w2v_g300[w])\n",
    "        rev_vec = np.array(rev_vec)\n",
    "        features.append(np.mean(rev_vec, axis =0))\n",
    "\n",
    "#     features = np.array(features) \n",
    "    return features\n",
    "X_train_w2v, X_test_w2v = get_features(X_train),get_features(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b692fdb",
   "metadata": {
    "id": "7b692fdb"
   },
   "source": [
    " ***Perceptron***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "308622de",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "308622de",
    "outputId": "32112414-fdaf-4bb3-d1ae-21a214ba19c9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Perceptron(warm_start=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Perceptron</label><div class=\"sk-toggleable__content\"><pre>Perceptron(warm_start=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Perceptron(warm_start=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_features = pd.DataFrame(X_train_w2v)\n",
    "\n",
    "train_features = pd.DataFrame(list(map(np.ravel, X_train_w2v)))\n",
    "train_features = train_features.fillna(0)\n",
    "test_features = pd.DataFrame(list(map(np.ravel, X_test_w2v)))\n",
    "test_features = test_features.fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "perceptron_classifier = Perceptron(warm_start=True)\n",
    "\n",
    "perceptron_classifier.fit(train_features,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5a60bbd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5a60bbd",
    "outputId": "a647cc1e-706f-4b44-acea-1b4e0dd0d807"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of perceptron is:  0.5688333333333333\n"
     ]
    }
   ],
   "source": [
    "preds = perceptron_classifier.predict(test_features)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Accuracy of perceptron is: \",accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad5b13f",
   "metadata": {},
   "source": [
    "```Accuracy value of perceptron using TF-IDF for feature extraction[HW-1] : 0.59 - 0.63```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90edfe4",
   "metadata": {
    "id": "b90edfe4"
   },
   "source": [
    "***SVM***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbe3aae2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fbe3aae2",
    "outputId": "73773afc-c748-4efd-9d63-3f0cbd0f60ae"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svc = LinearSVC()\n",
    "svc.fit(train_features,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9db639ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9db639ab",
    "outputId": "6428e35c-c35a-4717-f79c-795d07458867"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM is: 0.63475\n"
     ]
    }
   ],
   "source": [
    "preds = svc.predict(test_features)\n",
    "\n",
    "print(\"Accuracy of SVM is:\",accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b86d01e",
   "metadata": {},
   "source": [
    "```Accuracy value of SVM using TF-IDF for feature extraction[HW-1]: 0.67```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f2e780",
   "metadata": {},
   "source": [
    "**What do you conclude from comparing performances for the models\n",
    "trained using the two different feature types (TF-IDF and your trained Word2Vec features)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c661386d",
   "metadata": {},
   "source": [
    "Looking at the accuracy of the values on Perceptron as well as SVM, we can clearly see that using Word2Vec features perform at par with than how they performed using the TF-IDF features.\n",
    "\n",
    "During experimentation, sometimes Word2Vec performs a little better than TF-IDF and vice versa. The reason for this can be many, the data subset both the features get, pre-processing etc\n",
    "\n",
    "However, SVM performs better than Perceptron using TF-IDF as well as Word2Vec features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bb09ee",
   "metadata": {
    "id": "80bb09ee"
   },
   "source": [
    "## 4. Feedforward Neural Networks (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0b35e9",
   "metadata": {
    "id": "af0b35e9"
   },
   "source": [
    "(a) (10 points)\n",
    "To generate the input features, use the average Word2Vec vectors similar to\n",
    "the “Simple models” section and train the neural network. Report accuracy\n",
    "values on the testing split for your MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce817b4c",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3103d97",
   "metadata": {
    "id": "b3103d97"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aab994",
   "metadata": {
    "id": "78aab994"
   },
   "source": [
    "### Create DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae1911d",
   "metadata": {},
   "source": [
    "Both the train dataloader as well as the test dataloader return the tensors of the features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8ecc6d9",
   "metadata": {
    "id": "e8ecc6d9"
   },
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self,train_features,labels):\n",
    "        \n",
    "        self.train_features = train_features\n",
    "        \n",
    "            \n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.train_features)\n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "#         return torch.tensor(self.train_features[index]), torch.nn.functional.one_hot(torch.tensor(self.labels[index]),num_classes = 3)\n",
    "        return torch.tensor(self.train_features[index]), torch.tensor(self.labels[index])\n",
    "    \n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,test_features,labels):\n",
    "        \n",
    "        self.test_features = test_features\n",
    "        self.labels = labels\n",
    "     \n",
    "    def __len__(self):\n",
    "        return len(self.test_features)\n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        return torch.tensor(self.test_features[index]), torch.tensor(self.labels[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526d0991",
   "metadata": {},
   "source": [
    "For creating train and test data we are using the features that we used for Simple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d032c1e",
   "metadata": {
    "id": "1d032c1e"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_data = TrainDataset(train_features.values, y_train.values)\n",
    "test_data = TestDataset(test_features.values,y_test.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efb3fcf",
   "metadata": {},
   "source": [
    "After experimenting,the batch_size of 16 gives the best performance. So while creating the dataloaders, I used a batch_size of 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fa6da2c",
   "metadata": {
    "id": "1fa6da2c"
   },
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,num_workers=num_workers)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a579f74",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6313e1",
   "metadata": {},
   "source": [
    "I have used the activation as relu. I have also used dropout to prevent overfitting of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "978ace98",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "978ace98",
    "outputId": "0e49b650-fdde-4bc2-8c67-2059ded8226e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=300, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the NN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        hidden_1 = 100\n",
    "        hidden_2 = 10\n",
    "   \n",
    "        self.fc1 = nn.Linear(300, hidden_1)     \n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)   \n",
    "        self.fc3 = nn.Linear(hidden_2, 3)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "     \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "   \n",
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac512d3",
   "metadata": {},
   "source": [
    "CrossEntropyLoss performs well on multiclass classificaion. I have used the optimizer to be Adam with a learning rate of 0.01. I tried SGD optimizer as well but this performs the best.\n",
    "\n",
    "After experimenting, I used the number of epochs to be 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a0e6182",
   "metadata": {
    "id": "5a0e6182"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4f0b6ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4f0b6ab",
    "outputId": "d6d67be2-8d70-4a1f-d235-60f94c514ba8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.909088 \n",
      "Epoch: 2 \tTraining Loss: 0.856726 \n",
      "Epoch: 3 \tTraining Loss: 0.839670 \n",
      "Epoch: 4 \tTraining Loss: 0.829170 \n",
      "Epoch: 5 \tTraining Loss: 0.818103 \n",
      "Epoch: 6 \tTraining Loss: 0.811943 \n",
      "Epoch: 7 \tTraining Loss: 0.804650 \n",
      "Epoch: 8 \tTraining Loss: 0.798262 \n",
      "Epoch: 9 \tTraining Loss: 0.791184 \n",
      "Epoch: 10 \tTraining Loss: 0.787318 \n",
      "Epoch: 11 \tTraining Loss: 0.783119 \n",
      "Epoch: 12 \tTraining Loss: 0.777745 \n",
      "Epoch: 13 \tTraining Loss: 0.771641 \n",
      "Epoch: 14 \tTraining Loss: 0.768381 \n",
      "Epoch: 15 \tTraining Loss: 0.764289 \n",
      "Epoch: 16 \tTraining Loss: 0.762391 \n",
      "Epoch: 17 \tTraining Loss: 0.757201 \n",
      "Epoch: 18 \tTraining Loss: 0.754969 \n",
      "Epoch: 19 \tTraining Loss: 0.751023 \n",
      "Epoch: 20 \tTraining Loss: 0.746655 \n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "   \n",
    "    train_loss = 0.0\n",
    "   \n",
    "  \n",
    "    model.train() \n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.float())\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "   \n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} '.format(\n",
    "        epoch+1, \n",
    "        train_loss\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d06f2d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the FNN on the average word2vec: 63.24166666666667 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x,y in test_loader:\n",
    "        outputs = model(x.float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        \n",
    "\n",
    "    print('Test Accuracy of the FNN on the average word2vec: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc22ef75",
   "metadata": {
    "id": "bc22ef75"
   },
   "source": [
    "(b) (15 points)\n",
    "To generate the input features, concatenate the first 10 Word2Vec vectors\n",
    "for each review as the input feature (x = [WT\n",
    "1 , ..., WT\n",
    "10]) and train the neural\n",
    "network. Report the accuracy value on the testing split for your MLP model.\n",
    "What do you conclude by comparing accuracy values you obtain with\n",
    "those obtained in the “’Simple Models” section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9325b0",
   "metadata": {},
   "source": [
    "I have modified the get_features function previously to limit the words to 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "302b5432",
   "metadata": {
    "id": "302b5432"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_features_FNN(df):\n",
    "    \n",
    "    features = []\n",
    "    reviews = df.apply(lambda x: [w for w in x.split()])\n",
    "    \n",
    "    for review in reviews:\n",
    "        rev_vec = []\n",
    "        for w in review:\n",
    "\n",
    "            if w  in w2v_g300:\n",
    "                rev_vec.append(w2v_g300[w])\n",
    "            if len(rev_vec) == 10:\n",
    "              break\n",
    "                \n",
    "        if len(rev_vec) < 10:\n",
    "            for i in range(10-len(rev_vec)):\n",
    "                rev_vec.append([0]*300)\n",
    "        \n",
    "        features.append(np.concatenate(rev_vec))\n",
    "\n",
    "    return features\n",
    "X_train_w2v, X_test_w2v = get_features_FNN(X_train),get_features_FNN(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50893d29",
   "metadata": {
    "id": "50893d29"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_data = TrainDataset(X_train_w2v, y_train.values)\n",
    "test_data = TestDataset(X_test_w2v,y_test.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71d53ace",
   "metadata": {
    "id": "71d53ace"
   },
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,num_workers=num_workers)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4c90f2",
   "metadata": {},
   "source": [
    "The model architecture is similar, just the input length has changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7ac9ba1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c7ac9ba1",
    "outputId": "b358e3cf-af2c-4eae-b691-ddf754614a27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=3000, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        hidden_1 = 100\n",
    "        hidden_2 = 10\n",
    "    \n",
    "        self.fc1 = nn.Linear(3000, hidden_1)\n",
    "       \n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "       \n",
    "        self.fc3 = nn.Linear(hidden_2, 3)\n",
    "       \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "QOtbeQoSphXj",
   "metadata": {
    "id": "QOtbeQoSphXj"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "80df0ed2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "80df0ed2",
    "outputId": "e778e23a-8c16-4886-e364-5681d451a706"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.964631\n",
      "Epoch: 2 \tTraining Loss: 0.890520\n",
      "Epoch: 3 \tTraining Loss: 0.834062\n",
      "Epoch: 4 \tTraining Loss: 0.768162\n",
      "Epoch: 5 \tTraining Loss: 0.705709\n",
      "Epoch: 6 \tTraining Loss: 0.643937\n",
      "Epoch: 7 \tTraining Loss: 0.596140\n",
      "Epoch: 8 \tTraining Loss: 0.555489\n",
      "Epoch: 9 \tTraining Loss: 0.514732\n",
      "Epoch: 10 \tTraining Loss: 0.480082\n",
      "Epoch: 11 \tTraining Loss: 0.457447\n",
      "Epoch: 12 \tTraining Loss: 0.429844\n",
      "Epoch: 13 \tTraining Loss: 0.412119\n",
      "Epoch: 14 \tTraining Loss: 0.386939\n",
      "Epoch: 15 \tTraining Loss: 0.370552\n",
      "Epoch: 16 \tTraining Loss: 0.354751\n",
      "Epoch: 17 \tTraining Loss: 0.344845\n",
      "Epoch: 18 \tTraining Loss: 0.331737\n",
      "Epoch: 19 \tTraining Loss: 0.320827\n",
      "Epoch: 20 \tTraining Loss: 0.305609\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "   \n",
    "    train_loss = 0.0\n",
    "\n",
    "    \n",
    "   \n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.float())\n",
    "       \n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "      \n",
    "        \n",
    "        \n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e44a1039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the FNN model on the first 10 word2vec: 52.34166666666667 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x,y in test_loader:\n",
    "        outputs = model(x.float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        \n",
    "\n",
    "    print('Test Accuracy of the FNN model on the first 10 word2vec: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402e5d61",
   "metadata": {},
   "source": [
    "```What do you conclude by comparing accuracy values you obtain with\n",
    "those obtained in the “’Simple Models” section.```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1ffd06",
   "metadata": {},
   "source": [
    "Looking at the accuracy of Forward Neural Network with the simple models, I can conclude the following from my results:\n",
    "\n",
    "1. On the average Word2Vec features, the Feed Forward Neural Network gives a performance boost,\n",
    "2. Considering, just the first 10 words, the FNN model does not perform good. This is in accordance with the fact that we consider just the first few words. For different subsets of the data - the accuracy ranges in 50-60%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd290f3",
   "metadata": {
    "id": "cdd290f3"
   },
   "source": [
    "## 5. Recurrent Neural Networks (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c03948fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self,train_features,labels):\n",
    "        \n",
    "        self.train_features = train_features\n",
    "        \n",
    "            \n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.train_features)\n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        return torch.tensor(self.train_features[index]), torch.tensor(self.labels[index])\n",
    "    \n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,test_features,labels):\n",
    "        \n",
    "        self.test_features = test_features\n",
    "        self.labels = labels\n",
    "     \n",
    "    def __len__(self):\n",
    "        return len(self.test_features)\n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        return torch.tensor(self.test_features[index]), torch.tensor(self.labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "IfI_QmUwnuE4",
   "metadata": {
    "id": "IfI_QmUwnuE4"
   },
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "def get_features_RNN(df):\n",
    "    \n",
    "    features = []\n",
    "    reviews = df.apply(lambda x: [w for w in x.split()])\n",
    "    \n",
    "    for review in reviews:\n",
    "        rev_vec = []\n",
    "        for w in review:\n",
    "\n",
    "            if w  in w2v_g300:\n",
    "                rev_vec.append(w2v_g300[w])\n",
    "            if len(rev_vec) == 20:\n",
    "              break\n",
    "                \n",
    "        if len(rev_vec) < 20:\n",
    "            for i in range(20-len(rev_vec)):\n",
    "                rev_vec.append([0]*300)\n",
    "        \n",
    "        features.append(rev_vec)\n",
    "\n",
    "    return features\n",
    "X_train_w2v, X_test_w2v = get_features_RNN(X_train),get_features_RNN(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "EiM7FzVWZFAs",
   "metadata": {
    "id": "EiM7FzVWZFAs"
   },
   "outputs": [],
   "source": [
    "train_data = TrainDataset(X_train_w2v, y_train.values)\n",
    "test_data = TestDataset(X_test_w2v,y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "sLx1TBjFZTRT",
   "metadata": {
    "id": "sLx1TBjFZTRT"
   },
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,num_workers=num_workers)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cb932506",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNNModule(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, n_classes):\n",
    "        super(MyRNNModule, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, h = self.rnn(x)\n",
    "        h  = h.squeeze(0)\n",
    "        out = self.fc(h)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6334f10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = MyRNNModule(input_size=300, hidden_size=20, num_layers=1, n_classes=3)\n",
    "critertion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9fcb11ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46810/3896146456.py:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  return torch.tensor(self.train_features[index]), torch.tensor(self.labels[index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.9017\n",
      "Epoch 2, Loss: 0.8870\n",
      "Epoch 3, Loss: 0.9140\n",
      "Epoch 4, Loss: 0.7937\n",
      "Epoch 5, Loss: 0.7175\n",
      "Epoch 6, Loss: 1.0559\n",
      "Epoch 7, Loss: 0.9040\n",
      "Epoch 8, Loss: 0.8322\n",
      "Epoch 9, Loss: 1.1945\n",
      "Epoch 10, Loss: 0.9748\n",
      "Epoch 11, Loss: 0.7747\n",
      "Epoch 12, Loss: 0.9769\n",
      "Epoch 13, Loss: 0.8258\n",
      "Epoch 14, Loss: 0.7168\n",
      "Epoch 15, Loss: 1.0199\n",
      "Epoch 16, Loss: 0.8571\n",
      "Epoch 17, Loss: 0.6332\n",
      "Epoch 18, Loss: 0.8812\n",
      "Epoch 19, Loss: 0.7856\n",
      "Epoch 20, Loss: 0.9537\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(x)\n",
    "        loss = critertion(outputs, y)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print ('Epoch {}, Loss: {:.4f}'\n",
    "                     .format(epoch+1, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e9baecd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the RNN model: 59.725 %\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x,y in test_loader:\n",
    "        outputs = model(x)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        \n",
    "\n",
    "    print('Test Accuracy of the RNN model: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847acee8",
   "metadata": {},
   "source": [
    "```What do you conclude by comparing accuracy values you obtain with\n",
    "those obtained with feedforward neural network models.```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81bbf69",
   "metadata": {},
   "source": [
    "By comapring the accuracy values of RNN with FNN, there are two points to be noted:\n",
    "1. The performance of RNN as comapared to FNN with the average Word2Vec is less since the accuarcy value of FNN trained on average Word2Vec is more than RNN.\n",
    "2. The performance of RNN when compared to FNN with the first 10 Word2Vec concatenated is bettey since the accuarcy value of FNN trained on first 10 concatenated Word2Vec is less than RNN trained on first 20 concatenated Word2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84d2cbe",
   "metadata": {},
   "source": [
    "(b) (10 points)\n",
    "Repeat part (a) by considering a gated recurrent unit cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ea20d245",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGRUModule(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, n_classes):\n",
    "        super(MyGRUModule, self).__init__()\n",
    "        self.gru = torch.nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, h = self.gru(x)\n",
    "        h  = h.squeeze(0)\n",
    "        out = self.fc(h)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e3b3b684",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = MyGRUModule(input_size=300, hidden_size=20, num_layers=1, n_classes=3)\n",
    "critertion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a6fb6da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1Loss: 0.8753\n",
      "Epoch 2Loss: 0.8942\n",
      "Epoch 3Loss: 0.8456\n",
      "Epoch 4Loss: 0.5885\n",
      "Epoch 5Loss: 0.6874\n",
      "Epoch 6Loss: 0.7712\n",
      "Epoch 7Loss: 0.5330\n",
      "Epoch 8Loss: 0.6769\n",
      "Epoch 9Loss: 0.7032\n",
      "Epoch 10Loss: 0.5011\n",
      "Epoch 11Loss: 0.8105\n",
      "Epoch 12Loss: 0.6409\n",
      "Epoch 13Loss: 0.5740\n",
      "Epoch 14Loss: 0.7851\n",
      "Epoch 15Loss: 0.6700\n",
      "Epoch 16Loss: 0.8148\n",
      "Epoch 17Loss: 0.5101\n",
      "Epoch 18Loss: 1.0453\n",
      "Epoch 19Loss: 0.6948\n",
      "Epoch 20Loss: 0.6510\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(x)\n",
    "        loss = critertion(outputs, y)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print ('Epoch {}Loss: {:.4f}'\n",
    "                     .format(epoch+1, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2f8dcd55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the GRU model: 64.7 %\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x,y in test_loader:\n",
    "        outputs = model(x)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "#         correct += (list(int(p) for p in predicted) == y).sum().item()\n",
    "        \n",
    "\n",
    "    print('Test Accuracy of the GRU model: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7261e09d",
   "metadata": {},
   "source": [
    "Repeat part (a) by considering an LSTM unit cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "266fa793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTMModule(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, n_classes):\n",
    "        super(MyLSTMModule, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, h = self.lstm(x)\n",
    "        h  = h[1].squeeze(0)\n",
    "        out = self.fc(h)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eb23df48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = MyLSTMModule(input_size=300, hidden_size=20, num_layers=1, n_classes=3)\n",
    "critertion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "02a33b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.8822\n",
      "Epoch 2, Loss: 1.0440\n",
      "Epoch 3, Loss: 0.6420\n",
      "Epoch 4, Loss: 0.8885\n",
      "Epoch 5, Loss: 0.6598\n",
      "Epoch 6, Loss: 0.8232\n",
      "Epoch 7, Loss: 0.8359\n",
      "Epoch 8, Loss: 0.7274\n",
      "Epoch 9, Loss: 0.7394\n",
      "Epoch 10, Loss: 0.5944\n",
      "Epoch 11, Loss: 0.7048\n",
      "Epoch 12, Loss: 0.7006\n",
      "Epoch 13, Loss: 0.7930\n",
      "Epoch 14, Loss: 0.5196\n",
      "Epoch 15, Loss: 0.6895\n",
      "Epoch 16, Loss: 0.4376\n",
      "Epoch 17, Loss: 0.8017\n",
      "Epoch 18, Loss: 0.7053\n",
      "Epoch 19, Loss: 0.4792\n",
      "Epoch 20, Loss: 0.6742\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(x)\n",
    "        loss = critertion(outputs, y)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print ('Epoch {}, Loss: {:.4f}'\n",
    "                     .format(epoch+1, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4c959281",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the LSTM model: 64.89166666666667 %\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x,y in test_loader:\n",
    "        outputs = model(x)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "#         correct += (list(int(p) for p in predicted) == y).sum().item()\n",
    "        \n",
    "\n",
    "    print('Test Accuracy of the LSTM model: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96a5e85",
   "metadata": {},
   "source": [
    "```What do you conclude by comparing accuracy values you obtain by GRU,\n",
    "LSTM, and simple RNN.```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5977f95",
   "metadata": {},
   "source": [
    "The following conclusions can be made by comparing the accuarcy values obtained by GRU, LSTM and simple RNN:\n",
    "- GRU and LSTM perform  far better than RNN\n",
    "- The performance of GRU and LSTM is comaparable although LSTM performs a bit better than GRU"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
